{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Are UK families well fed? What predicts it?\n",
    "# Lucas Gonzalez Santa Cruz - Data Science Intensive - Sept 2016\n",
    "\n",
    "# ------------- O U T L I N E ---------------\n",
    "# - Introduction\n",
    "# - Get the files from the web\n",
    "# - Households\n",
    "# - Diaries\n",
    "# - Nutrients\n",
    "# - Score / Classification\n",
    "# - Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------- I N T R O D U C T I O N ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA:\n",
    "# UK families (each with adult males and females, and children) wrote down what food they bought over a week.\n",
    "# - 6000 families (with socio-economic descriptors).\n",
    "# - 315 foods (each with quantities and 47 nutrients) in 18 food-groups.\n",
    "# === 1974-2000:\n",
    "# visualisation: http://britains-diet.labs.theodi.org/\n",
    "# overview: http://theodi.org/blog/family-food-publication-day-blog-feeding-the-hunger-for-data\n",
    "# survey methodology: https://www.gov.uk/government/publications/family-food-methodology\n",
    "# === 2001-etc:\n",
    "# not open data: https://www.gov.uk/government/statistical-data-sets/family-food-datasets\n",
    "\n",
    "# GOALS:\n",
    "# Compute a nutritional \"score\" (higher is better) or \"category\" (well/badly).\n",
    "# Build a regression/classification model to look for predictive or explanatory variables.\n",
    "\n",
    "# SCORE (CAN BE CATEGORISED):\n",
    "# Ratio between (a) calories bought per person and per day\n",
    "# and (b) recommended calories for each family (say one female, one male, and 2 children).\n",
    "# (When \"calories\" are done, \"protein\" etc can be tried, and more complex models built.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "data2000 = 'data/NFSopen_2000/'\n",
    "dataRefe = 'data/NFSopen_Reference/'\n",
    "pd.options.display.width = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------- H O U S E H O L D S ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house</th>\n",
       "      <th>men</th>\n",
       "      <th>women</th>\n",
       "      <th>children</th>\n",
       "      <th>calories_rec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>261117</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>261118</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>261119</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>261120</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>9900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>261121</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    house  men  women  children  calories_rec\n",
       "0  261117    2      1         0          7000\n",
       "1  261118    1      0         0          2500\n",
       "2  261119    1      0         0          2500\n",
       "3  261120    1      1         3          9900\n",
       "4  261121    0      1         0          2000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In 2000, 53 variables from 6699 households were collected.\n",
    "df_house2000 = pd.DataFrame.from_csv(data2000+'2000 household data.txt', sep='\\t', index_col=None)\n",
    "#df_house2000.shape #(6699, 53)\n",
    "# 'hhno', 'gormet2', 'reg', 'lad', 'styr', 'stmth', 'mic', 'frez', 'owndw', 'memhh',\n",
    "# 'schmilk', 'incgp', 'occhoh', 'sochoh', 'dacthoh', 'Finc_decile_by_members_of_hh',\n",
    "# 'finctp', 'pernohoh', 'pernohw', 'agehoh_banded', 'agehw_banded', 'adltm', 'adltf',\n",
    "# 'child', 'oaps', 'adltgt64', 'stqtr', 'country', 'gor', 'agemdk_banded', 'incgpa',\n",
    "# 'hhcomp', 'hhcompa', 'hcxigs', 'dat1rec', 'doormilk', 'frij', 'earners', 'occxhoh',\n",
    "# 'socxhoh', 'dactxhoh', 'szwkest', 'empst', 'wkdy1rec', 'vegind', 'gornw', 'pernxhoh',\n",
    "# 'jobsthoh', 'incgp745', 'withjob', 'landlord', 'furnish', 'benefits'\n",
    "\n",
    "#This is the description of each of the 53 fields:\n",
    "df_house_fields = pd.DataFrame.from_csv(dataRefe+'house-fields.txt', sep='\\t', index_col=None)\n",
    "df_house_fields.columns=['table', 'field', 'field_text', 'type1', 'size', 'type2', 'lookup', 'notes']\n",
    "#df_house_fields.drop(['table', 'type1', 'size', 'type2'], inplace=True, axis=1)\n",
    "df_house_fields = df_house_fields[['field', 'field_text', 'lookup', 'notes']]\n",
    "#df_house_fields.head()\n",
    "#df_house_fields.shape #(53, 4)\n",
    "\n",
    "#Family composition:\n",
    "#hhno: household number\n",
    "#adltm: number of male adults (18 years or over plus 16 or 17 head of households)\n",
    "#adltf: number of female adults (18 years or over plus 16 or 17 head of households)\n",
    "#child: number of children\n",
    "df_house = df_house2000[['hhno', 'adltm', 'adltf', 'child']]\n",
    "df_house.columns=['house', 'men', 'women', 'children']\n",
    "#df_house.head()\n",
    "\n",
    "#Calories recommended:\n",
    "# http://www.foodlabel.org.uk/label/gda_values.aspx#item1\n",
    "# Guideline Daily Amount Values Typical values\n",
    "# Women \tMen \tChildren (5-10 years)\n",
    "# Calories \t2,000 kcal \t2,500 kcal \t1,800 kcal\n",
    "# There are also recommendations for Protein, Carbohydrate, Sugars, Fat, Saturates, Fibre and Salt\n",
    "df_house['calories_rec'] = (df_house.women * 2000) +  (df_house.men * 2500) + (df_house.children * 1800)\n",
    "df_house.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Many of the household fields have auxiliary files with (code, text).\n",
    "\n",
    "# I wrote this to read the auxiliary files:\n",
    "\"\"\"\n",
    "#df_matrix = df_house_fields.as_matrix()\n",
    "#for line in df_matrix:\n",
    "#    #print line[0], line[2]\n",
    "#    #print \"*\"+str(line[2])+\"*\"\n",
    "#    if str(line[2]) != \"nan\" :\n",
    "#        filename = str(line[2])\n",
    "#        filename = filename.replace(\":\", \"_\")\n",
    "#        print \"#\" + line[1]\n",
    "#        print \"df_\" + line[0] + \" = pd.DataFrame.from_csv(dataRefe+'\" + filename + \"', sep='\\\\t', index_col=None)\"\n",
    "#        print \"#df_\" + line[0] + \".columns=['','']\"\n",
    "#        print \"#df_\" + line[0] + \".shape\"\n",
    "#        print \"#df_\" + line[0] + \".head(5)\"\n",
    "#        print\n",
    "\"\"\"\n",
    "\n",
    "# Then, I fixed the resulting code (below): uncommented some fields, fixed some by hand.\n",
    "\n",
    "#government office region metropolitan county = 27 x (code, text)\n",
    "#df_gormet2 = pd.DataFrame.from_csv(dataRefe+'DCV_ GORMET2.txt', sep='\\t', index_col=None)\n",
    "#df_gormet2.columns=['gormet', 'gormet_text']\n",
    "\n",
    "#standard statistical region = 11 x (code, text); Scotland = 1\n",
    "df_reg = pd.DataFrame.from_csv(dataRefe+'DCV_ Region.txt', sep='\\t', index_col=None)\n",
    "df_reg.columns=['region','region_text']\n",
    "\n",
    "#local authority district = 457 x (code, text)\n",
    "#df_lad = pd.DataFrame.from_csv(dataRefe+'DCV_ LAD.txt', sep='\\t', index_col=None)\n",
    "#df_lad.columns=['lad','lad_text']\n",
    "\n",
    "#ownership of microwave (1 = yes 0= not)\n",
    "df_mic = pd.DataFrame.from_csv(dataRefe+'DCV_ Microwave ownership.txt', sep='\\t', index_col=None)\n",
    "df_mic.columns=['microwave','microwave_text']\n",
    "\n",
    "#ownership of freezer (1 = yes 0= not)\n",
    "df_frez = pd.DataFrame.from_csv(dataRefe+'DCV_ Freezer ownership.txt', sep='\\t', index_col=None)\n",
    "df_frez.columns=['freezer','freezer_text']\n",
    "\n",
    "#tenure (ownership of dwelling) = 7 categories\n",
    "df_owndw = pd.DataFrame.from_csv(dataRefe+'DCV_ Ownership of dwelling.txt', sep='\\t', index_col=None)\n",
    "df_owndw.columns=['own_dwelling','own_dwelling_text']\n",
    "\n",
    "#school milk #yes, no\n",
    "df_schmilk = pd.DataFrame.from_csv(dataRefe+'DCV_ School milk.txt', sep='\\t', index_col=None)\n",
    "df_schmilk.columns=['school_milk','school_milk_text']\n",
    "\n",
    "#income group of the head of the household = 8 categories\n",
    "#df_incgp = pd.DataFrame.from_csv(dataRefe+'DCV_ Income Group.txt', sep='\\t', index_col=None)\n",
    "\n",
    "#occupation of the head of the household = 193 categories, 3 columns\n",
    "#df_occhoh = pd.DataFrame.from_csv(dataRefe+'DCV_ OCC_OCCHOH Occupation class with unpacked descriptions.txt', sep='\\t', index_col=None)\n",
    "\n",
    "#social occupation class of the head of the household = 380 categories\n",
    "#df_sochoh = pd.DataFrame.from_csv(dataRefe+'DCV_ Standard occupation class.txt', sep='\\t', index_col=None)\n",
    "#df_sochoh.columns=['hhh_job','hhh_job_text']\n",
    "\n",
    "#degree of activity of the head of the household = sedentary, active, moderate\n",
    "df_dacthoh = pd.DataFrame.from_csv(dataRefe+'DCV_ Degree of activity.txt', sep='\\t', index_col=None)\n",
    "df_dacthoh.columns=['head_sedentary','head_sedentary_text']\n",
    "\n",
    "#family income type #how we know about income: net, gross, estimated\n",
    "#df_finctp = pd.DataFrame.from_csv(dataRefe+'DCV_ Family income tp.txt', sep='\\t', index_col=None)\n",
    "#df_finctp.columns=['finctp','finctp_text']\n",
    "\n",
    "#age of head of household (banded) = 7 age groups\n",
    "df_agehoh_banded = pd.DataFrame.from_csv(dataRefe+'Ref_Age_bands_for_household_table.txt', sep='\\t', index_col=None)\n",
    "df_agehoh_banded.columns=['age_headhouse','age_headhouse_text']\n",
    "\n",
    "#age of housewife (banded) = 7 age groups\n",
    "df_agehw_banded = pd.DataFrame.from_csv(dataRefe+'Ref_Age_bands_for_household_table.txt', sep='\\t', index_col=None)\n",
    "df_agehw_banded.columns=['age_housewife','age_housewife_text']\n",
    "\n",
    "#government office region = 13 regions, Scotland is 12\n",
    "df_gor = pd.DataFrame.from_csv(dataRefe+'DCV_ GOR.txt', sep='\\t', index_col=None)\n",
    "df_gor.columns=['gor','gor_text']\n",
    "\n",
    "#age of main diary keeper = 7 age groups\n",
    "#df_agemdk_banded = pd.DataFrame.from_csv(dataRefe+'Ref_Age_bands_for_household_table.txt', sep='\\t', index_col=None)\n",
    "\n",
    "#income group (1 = All A  2=B  3=C 4=D & E2 5=E1 & OAPs)\n",
    "#df_incgpa = pd.DataFrame.from_csv(dataRefe+'DCV_ Income group (A group).txt', sep='\\t', index_col=None)\n",
    "#df_incgpa.columns=['income_group','income_group_text']\n",
    "\n",
    "#household composition = number of adults (male and female) and children\n",
    "df_hhcomp = pd.DataFrame.from_csv(dataRefe+'DCV_ Household Composition.txt', sep='\\t', index_col=None)\n",
    "df_hhcomp.columns=['hhcomp','hhcomp_text', 'hhcomp_text_long']\n",
    "\n",
    "#household composition (all adults) = number of adults and children - NOT USEFUL IMHO\n",
    "#df_hhcompa = pd.DataFrame.from_csv(dataRefe+'DCV_ Household Compositon (A group).txt', sep='\\t', index_col=None)\n",
    "#df_hhcompa.columns=['adults_children','adults_children_text']\n",
    "\n",
    "#household composition by income group = 28 categories, how many family members are this or that\n",
    "#df_hcxigs = pd.DataFrame.from_csv(dataRefe+'DCV_ Household composition x income group.txt', sep='\\t', index_col=None)\n",
    "#df_hcxigs.columns=['hh_comp_income','hh_comp_income_text']\n",
    "\n",
    "#doorstep milk delivery (1 = yes 2 = no)\n",
    "df_doormilk = pd.DataFrame.from_csv(dataRefe+'DCV_ Doormilk.txt', sep='\\t', index_col=None)\n",
    "df_doormilk.columns=['doormilk','doormilk_text']\n",
    "\n",
    "#ownership of fridge  (1 = yes 0= not)\n",
    "df_frij = pd.DataFrame.from_csv(dataRefe+'DCV_ Fridge ownership.txt', sep='\\t', index_col=None)\n",
    "df_frij.columns=['fridge','fridge_text']\n",
    "\n",
    "#occupation of crossover head of household\n",
    "#df_occxhoh = pd.DataFrame.from_csv(dataRefe+'DCV_ Occupation class.txt', sep='\\t', index_col=None)\n",
    "#df_occxhoh.columns=['occupation_class','occupation_class_text']\n",
    "\n",
    "#social occupation class of cross head of household #kinds of job\n",
    "#df_socxhoh = pd.DataFrame.from_csv(dataRefe+'DCV_ Standard occupation class.txt', sep='\\t', index_col=None)\n",
    "#df_socxhoh.columns=['socxhoh','socxhoh_text']\n",
    "\n",
    "#degree of activity of cross over head of household #sedentary etc\n",
    "#df_dactxhoh = pd.DataFrame.from_csv(dataRefe+'DCV_ Degree of activity.txt', sep='\\t', index_col=None)\n",
    "#df_dactxhoh.columns=['deg_activity','deg_activity_text']\n",
    "\n",
    "#size of work establishment of head of household #sme etc\n",
    "#df_szwkest = pd.DataFrame.from_csv(dataRefe+'DCV_ Size of work establishment.txt', sep='\\t', index_col=None)\n",
    "#df_szwkest.columns=['hoh_job_size','hoh_job_size_text']\n",
    "\n",
    "#employment status of head of household (1 = employed 2 = unemployed)\n",
    "#df_empst = pd.DataFrame.from_csv(dataRefe+'DCV_ Employment status.txt', sep='\\t', index_col=None)\n",
    "#df_empst.columns=['hoh_employed','hoh_employed_text']\n",
    "\n",
    "#weekday (1 st day of recording) 1 = Sunday....7=Saturday\n",
    "#df_wkdy1rec = pd.DataFrame.from_csv(dataRefe+'DCV_ Weekday (1st day of recording).txt', sep='\\t', index_col=None)\n",
    "#df_wkdy1rec.columns=['weekday','weekday_text']\n",
    "\n",
    "#vegetarian indicator\n",
    "df_vegind = pd.DataFrame.from_csv(dataRefe+'DCV_ Vegetarian indicator.txt', sep='\\t', index_col=None)\n",
    "df_vegind.columns=['vegetarian','vegetarian_text']\n",
    "\n",
    "#government office region for northwest and merseyside (1=north west/merseyside 2=other)\n",
    "#df_gornw = pd.DataFrame.from_csv(dataRefe+'DCV_ GORNW.txt', sep='\\t', index_col=None)\n",
    "#FILE DOES NOT EXIST -- CHECK\n",
    "\n",
    "#job status of head of household\n",
    "df_jobsthoh = pd.DataFrame.from_csv(dataRefe+'DCV_ Job status of HOH.txt', sep='\\t', index_col=None)\n",
    "df_jobsthoh.columns=['hohjobstatus','hohjobstatus_text']\n",
    "\n",
    "#income group (1974-1975)\n",
    "#df_incgp745 = pd.DataFrame.from_csv(dataRefe+'DCV_ Income group 7475.txt', sep='\\t', index_col=None)\n",
    "#df_incgp745.columns=['income_group','income_group_text']\n",
    "\n",
    "#with job indicator - accomodation tied to employment\n",
    "df_withjob = pd.DataFrame.from_csv(dataRefe+'DCV_ WITHJOB (1998 to 2000 only).txt', sep='\\t', index_col=None)\n",
    "df_withjob.columns=['withjob','withjob_text']\n",
    "\n",
    "#landlord status indicator: type of landlord\n",
    "#df_landlord = pd.DataFrame.from_csv(dataRefe+'DCV_ LANDLORD (1998 to 2000 only).txt', sep='\\t', index_col=None)\n",
    "#df_landlord.columns=['landlord','landlord_text']\n",
    "\n",
    "#furnished indicator: is the accomodation furnished\n",
    "#df_furnish = pd.DataFrame.from_csv(dataRefe+'DCV_ FURNISHED (1998 to 2000 only).txt', sep='\\t', index_col=None)\n",
    "#FILE DOES NOT EXIST -- CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------- D I A R I E S : FOOD BOUGHT BY THE HOUSEHOLDS DURING ONE WEEK ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "family - qty, units, food, group) - details:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hhno</th>\n",
       "      <th>quantity</th>\n",
       "      <th>units</th>\n",
       "      <th>minor_text</th>\n",
       "      <th>group_text</th>\n",
       "      <th>minor</th>\n",
       "      <th>major</th>\n",
       "      <th>major_text</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>261119</td>\n",
       "      <td>6.77</td>\n",
       "      <td>oz</td>\n",
       "      <td>SOUPS  DEHYDRATED  &amp;  POWDERED</td>\n",
       "      <td>ALL OTHER FOODS</td>\n",
       "      <td>31901</td>\n",
       "      <td>319</td>\n",
       "      <td>SOUPS DEHYDRATED AND POWDERED</td>\n",
       "      <td>314339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>261119</td>\n",
       "      <td>28.21</td>\n",
       "      <td>oz</td>\n",
       "      <td>BREAD WHOLEMEAL SLICED</td>\n",
       "      <td>ALL BREAD</td>\n",
       "      <td>26001</td>\n",
       "      <td>260</td>\n",
       "      <td>BREAD WHOLEMEAL SLICED</td>\n",
       "      <td>251263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>261119</td>\n",
       "      <td>1.75</td>\n",
       "      <td>pt</td>\n",
       "      <td>SEMI AND OTHER SKIMMED MILKS</td>\n",
       "      <td>OTHER MILK &amp; CREAM</td>\n",
       "      <td>1503</td>\n",
       "      <td>15</td>\n",
       "      <td>SKIMMED MILKS</td>\n",
       "      <td>9017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     hhno  quantity units                      minor_text          group_text  minor  major                     major_text   group\n",
       "0  261119      6.77    oz  SOUPS  DEHYDRATED  &  POWDERED     ALL OTHER FOODS  31901    319  SOUPS DEHYDRATED AND POWDERED  314339\n",
       "1  261119     28.21    oz          BREAD WHOLEMEAL SLICED           ALL BREAD  26001    260         BREAD WHOLEMEAL SLICED  251263\n",
       "2  261119      1.75    pt    SEMI AND OTHER SKIMMED MILKS  OTHER MILK & CREAM   1503     15                  SKIMMED MILKS    9017"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6k+ families recorded their food purchases for a week. One line per food-item: 3 pints of milk, etc.\n",
    "df_diary = pd.DataFrame.from_csv(data2000+'2000 diary data.txt', sep='\\t', index_col=None)\n",
    "df_diary.columns=['hhno', 'fooditem', 'logday', 'purchasevalue', 'minor', 'quantity', 'purchasefree']\n",
    "#hhno = household number\n",
    "#fooditem = food item number: if on a given day you buy bread and milk, one would be 1 and the other 2.\n",
    "#logday (1 equals the first day of record keeping)\n",
    "#purchase value (pence)\n",
    "#minor = 315 detailed food codes\n",
    "#quantity: available for consumption (includes bones etc, which are not consumed).\n",
    "#          In imperial numbers except confectionery and alcohol, in metric units 1992-2000.\n",
    "#purchasefree: \"1,5 & 9 indicate purchased food 2, 3, 4, 6, 7 & 8 indicate free food.\"\n",
    "df_diary = df_diary[['hhno', 'minor', 'quantity']]\n",
    "\n",
    "# 315 detailed \"minor\" foodcodes --- then aggregated into 183 \"major\" groups.\n",
    "df_min_maj = pd.DataFrame.from_csv(dataRefe+'Ref_ Minor and major foods.txt', sep='\\t', index_col=None)\n",
    "df_min_maj.columns=['minor', 'minor_text', 'major']\n",
    "df_maj_text = pd.DataFrame.from_csv(dataRefe+'Ref_ Major food codes.txt', sep='\\t', index_col=None)\n",
    "df_maj_text.columns=['major', 'major_text']\n",
    "#Each \"foodcode\" was described with one of 7 \"units\": pints, ounces, etc.\n",
    "df_min_units = pd.DataFrame.from_csv(dataRefe+'Ref_MINFD_Minor_food_codes.txt', sep='\\t', index_col=None)\n",
    "df_min_units.columns=['minor','minor_text','units']\n",
    "df_min_units.drop(['minor_text'], inplace=True, axis=1) #to avoid duplicate later\n",
    "\n",
    "#24 more aggregated \"groups\" were defined, and the 183 previous \"detailed groups\" were mapped to these 24.\n",
    "df_grp_text = pd.DataFrame.from_csv(dataRefe+'Ref_ food groups (standard).txt', sep='\\t', index_col=None)\n",
    "df_grp_text.columns=['group','group_text']\n",
    "df_mapping = pd.DataFrame.from_csv(dataRefe+'Ref_ Major-food group mapping.txt', sep='\\t', index_col=None)\n",
    "df_mapping.columns=['major','group']\n",
    "#The mapping included 92 groups, and those 24 had to be selected.\n",
    "group24 = [4006, 9017, 22023, 31041, 46094, 100127, 129129, 135148, 150154, 155161, 162171, 172183, \\\n",
    "           184208, 210231, 233248, 251263, 264264, 267277, 281301, 304313, 314339, 340344, 350354, 380389]\n",
    "df_maj_group = df_mapping[df_mapping['group'].isin(group24) == True]\n",
    "\n",
    "#Food diaries were merged into one table:\n",
    "df = pd.merge(df_diary, df_min_maj, how='left', on='minor')\n",
    "df = pd.merge(df, df_min_units, how='left', on='minor')\n",
    "df = pd.merge(df, df_maj_text, how='left', on='major')\n",
    "df = pd.merge(df, df_maj_group, how='left', on='major')\n",
    "df_diary = pd.merge(df, df_grp_text, how='left', on='group')\n",
    "df_diary = df_diary[['hhno', 'quantity', 'units', 'minor_text', 'group_text', 'minor', 'major', 'major_text', 'group']]\n",
    "print \"family - qty, units, food, group) - details:\"\n",
    "df_diary.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------- N U T R I E N T S ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice how the NCV is different for each quarter (of 2000), so later we'll have to average them\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>minor_text</th>\n",
       "      <th>nutr_text</th>\n",
       "      <th>quarter</th>\n",
       "      <th>nutconvfactor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MILK  LIQUID  FULL PRICE</td>\n",
       "      <td>Animal Protein</td>\n",
       "      <td>1</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MILK  LIQUID  FULL PRICE</td>\n",
       "      <td>Animal Protein</td>\n",
       "      <td>2</td>\n",
       "      <td>19.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MILK  LIQUID  FULL PRICE</td>\n",
       "      <td>Animal Protein</td>\n",
       "      <td>3</td>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MILK  LIQUID  FULL PRICE</td>\n",
       "      <td>Animal Protein</td>\n",
       "      <td>4</td>\n",
       "      <td>19.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MILK  LIQUID  FULL PRICE</td>\n",
       "      <td>Fat</td>\n",
       "      <td>1</td>\n",
       "      <td>22.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MILK  LIQUID  FULL PRICE</td>\n",
       "      <td>Fat</td>\n",
       "      <td>2</td>\n",
       "      <td>21.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MILK  LIQUID  FULL PRICE</td>\n",
       "      <td>Fat</td>\n",
       "      <td>3</td>\n",
       "      <td>22.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MILK  LIQUID  FULL PRICE</td>\n",
       "      <td>Fat</td>\n",
       "      <td>4</td>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MILK  LIQUID  FULL PRICE</td>\n",
       "      <td>Saturates</td>\n",
       "      <td>1</td>\n",
       "      <td>13.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 minor_text       nutr_text  quarter  nutconvfactor\n",
       "0  MILK  LIQUID  FULL PRICE  Animal Protein        1          17.00\n",
       "1  MILK  LIQUID  FULL PRICE  Animal Protein        2          19.00\n",
       "2  MILK  LIQUID  FULL PRICE  Animal Protein        3          20.00\n",
       "3  MILK  LIQUID  FULL PRICE  Animal Protein        4          19.00\n",
       "4  MILK  LIQUID  FULL PRICE             Fat        1          22.00\n",
       "5  MILK  LIQUID  FULL PRICE             Fat        2          21.00\n",
       "6  MILK  LIQUID  FULL PRICE             Fat        3          22.00\n",
       "7  MILK  LIQUID  FULL PRICE             Fat        4          23.00\n",
       "8  MILK  LIQUID  FULL PRICE       Saturates        1          13.89"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We have the 47 nutrients (txt from a PDF)\n",
    "df_nut_names = pd.DataFrame.from_csv(dataRefe+'Nutrient Conversion factors.txt', sep=',', index_col=None)\n",
    "df_nut_names.columns=['nutrient','nutr_text', 'nutr_comments']\n",
    "df_nut_names.head(8) # we're interested in nutrient = 8 (Energy - Kcal)\n",
    "#We have the conversion factors for each 315 'minor', for each of the 4 quarters of year 2000\n",
    "df_nut2000 = pd.DataFrame.from_csv(data2000+'2000 nutrient conversion factors.txt', sep='\\t', index_col=None)\n",
    "df_nut2000.columns=['minor', 'nutrient', 'year', 'quarter', 'nutconvfactor']\n",
    "#We merge the nutrients with the conversion factors\n",
    "df_nut = pd.merge(df_nut2000, df_nut_names, how='left', on='nutrient')\n",
    "df_nut = pd.merge(df_nut, df_min_maj, how='left', on='minor')\n",
    "df_nut.drop(['major'], inplace=True, axis=1) #to avoid duplicate later\n",
    "#.shape #(43348, 8) (some of the 'minor' had no nutrients)\n",
    "#'minor', 'nutrient', 'year', 'quarter', 'nutconvfactor', 'nutr_text', 'nutr_comments', 'minor_text'\n",
    "print \"Notice how the NCV is different for each quarter (of 2000), so later we'll have to average them\"\n",
    "df_nut[['minor_text', 'nutr_text', 'quarter', 'nutconvfactor']].head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "      <th>q3</th>\n",
       "      <th>q4</th>\n",
       "      <th>calories</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>380.00</td>\n",
       "      <td>370.00</td>\n",
       "      <td>380.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>380.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>404.38</td>\n",
       "      <td>374.53</td>\n",
       "      <td>382.8</td>\n",
       "      <td>399.8</td>\n",
       "      <td>390.3775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>404.38</td>\n",
       "      <td>374.53</td>\n",
       "      <td>382.8</td>\n",
       "      <td>399.8</td>\n",
       "      <td>390.3775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           q1      q2     q3     q4  calories\n",
       "minor                                        \n",
       "401    380.00  370.00  380.0  390.0  380.0000\n",
       "402    404.38  374.53  382.8  399.8  390.3775\n",
       "403    404.38  374.53  382.8  399.8  390.3775"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_nut[df_nut.nutrient == 8] #Select nutrient = Calories\n",
    "df = df.pivot(index='minor', columns='quarter', values='nutconvfactor')\n",
    "df.columns.name = None\n",
    "df.columns = ['q1', 'q2', 'q3', 'q4']\n",
    "df['calories'] = (df.q1 + df.q2 + df.q3 + df.q4) / 4 #average all the calories for the 4 quarters of 2000\n",
    "df_min_cal = df\n",
    "df_min_cal.head(3) #307, not 315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'minor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-bf85d3aa4388>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_food\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_food\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'minor'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#read calories for each food\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdf_min_cal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_min_cal\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'minor'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mdf_cal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_min_cal\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'minor'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'calories'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#merge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/lucas/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1995\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1996\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1997\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1999\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/lucas/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2002\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2003\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2004\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/lucas/anaconda2/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1348\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/lucas/anaconda2/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3289\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3290\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3291\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3292\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/lucas/anaconda2/lib/python2.7/site-packages/pandas/indexes/base.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   1945\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1946\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1947\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1949\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4154)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4018)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12368)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12322)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'minor'"
     ]
    }
   ],
   "source": [
    "#read foods\n",
    "df_min_maj = pd.DataFrame.from_csv(dataRefe+'Ref_ Minor and major foods.txt', sep='\\t', index_col=None)\n",
    "df_min_maj.columns=['minor', 'minor_text', 'major']\n",
    "df_food = df_min_maj\n",
    "df_food.index = df_food['minor']\n",
    "#read calories for each food\n",
    "df_min_cal.index = df_min_cal['minor']\n",
    "df_cal = df_min_cal[['minor', 'calories']]\n",
    "#merge\n",
    "df_food_cal = pd.merge(df_food, df_cal, how='left', on='minor')\n",
    "df_food.head(55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.merge(df3, df_diary, how='left', on='minor')\n",
    "df[['minor', 'quantity', 'calories']].head(5)\n",
    "df_cal_consumed = df\n",
    "df_cal_consumed['calconsumed'] = df_cal_consumed.quantity * df_cal_consumed.calories\n",
    "df_cal_consumed[['minor', 'quantity', 'calories', 'calconsumed']].head(5)\n",
    "df_cal_consumed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------- S C O R E / C L A S S I F I C A T I O N ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_cal_consumed has the calories consumed in detail: by hh and by fooditem\n",
    "#i'd like to sum that up into df_cal_household, with all the calories consumed by each family\n",
    "#currently i have no idea how to do that ... groupby then aggregate ?????\n",
    "\n",
    "import numpy as np\n",
    "df_cal_household = df_cal_consumed.groupby('hhno')\n",
    "print len(df_cal_consumed)\n",
    "print len(df_cal_household)\n",
    "df_cal_household.aggregate(np.sum) # DOESN'T WORK !!! TIME FOR A REST !!!\n",
    "#df_cal_household.groupby('calories', as_index=False).sum()\n",
    "\n",
    "sum_cal = df_cal_consumed.groupby('hhno')['calories'].sum()\n",
    "sum_cal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_cal_rec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now i only have to join them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------- M O D E L S ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------- R A N D O M   S T U F F   A N D   E X P E R I M E N T S ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#a) convert units to grams\n",
    "#b) select the number of family members\n",
    "#c) calculate \"grams of each food-group _per person_\" for each broad age-group\n",
    "#d) Look at the cells below to re-sort the food-group categories\n",
    "#e) Find how to add extra text that sits outside the cells\n",
    "#f) Re-write the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_houses = len(df_house2000)\n",
    "print \"Num of households: \", num_houses\n",
    "num_scotland = len(df_house2000[df_house2000.reg == 1])\n",
    "print \"Num of households in Scotland: \", num_scotland\n",
    "print \"Proportion of questionaires from Scotland: \", (num_scotland*100) / num_houses, \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_hhcomp.columns=['hhcomp', 'hhcomp_text', 'hhcomp_text_long']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_age_comp = pd.merge(df_house2000, df_hhcomp, how='left', on='hhcomp')\n",
    "df_age_comp = df_age_comp[['hhno', 'hhcomp', 'hhcomp_text', 'hhcomp_text_long']]\n",
    "\n",
    "#i ___thought___ i need to create variables \"adults\", \"children\" and then\n",
    "#if hh_comp = 1 then adult = 1, children = 0, etc\n",
    "#df_hhcomp #this 2, 7, 9, 10 and 11 are indeterminate - how many are like this?\n",
    "\n",
    "df_indeterminate = df_age_comp[df_age_comp.hhcomp.isin([2, 7, 9, 10, 11])]\n",
    "#df_indeterminate.shape #(51498, 4)\n",
    "print (len(df_indeterminate)*100/len(df_age_comp)), \"% of households (\", \\\n",
    "       len(df_indeterminate), \"of\", len(df_age_comp), \") have an indeterminate number of people (x or more)\", "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test=df_age_comp.groupby('hhcomp_text_long')\n",
    "test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test=df_indeterminate.groupby('hhcomp_text_long')\n",
    "test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#So what could we assume?\n",
    "#In theory:\n",
    "#A=1, C=? ... C=average number of children in families with A=1 ... for convenience C=1\n",
    "#A=2, C>3 ... C=average number of children in families with A=2 and C=4, 5, etc ... for convenience C=4\n",
    "#A>2, C=1or2 ... A=average number of adults in families with C=1or2, C=average number of children in families with C=1or2\n",
    "#A>2, C>2 ... A=average number of adults in families with C=3, 4, etc; C=average number of children in families with A=3, 4, etc\n",
    "#A>3 ... A=average number of adults in families with C=0\n",
    "#But in practice we don't have those details in our data. So, could we have that from national statistics?\n",
    "#Number of children in monoparental families in the UK ...\n",
    "#http://webarchive.nationalarchives.gov.uk/20160105160709/http://www.ons.gov.uk/ons/rel/family-demography/family-size/2012/family-size-rpt.html\n",
    "#For convenience (real people will be that or more -> real food/person will be _slightly lower_ than our estimates):\n",
    "#A=1, C=? ... C=1\n",
    "#A=2, C>3 ... C=4\n",
    "#A>2, C=1or2 ... A=3\n",
    "#A>2, C>2 ... A=3, C=3\n",
    "#A>3 ... A=4\n",
    "\n",
    "#import numpy as np\n",
    "#dftest = df\n",
    "#dftest['adults'] = np.where(dftest['hhcomp']==1, 1, 0)\n",
    "#dftest.head(5)\n",
    "#['adults', 'children'] #add variables?\n",
    "\n",
    "def get_adults (row): #2531\n",
    "    if row['hhcomp'] == 1:\n",
    "        return 1\n",
    "    if row['hhcomp'] == 2 :\n",
    "        return 1\n",
    "    if row['hhcomp'] == 3 :\n",
    "        return 2\n",
    "    if row['hhcomp'] == 4 :\n",
    "        return 2\n",
    "    if row['hhcomp'] == 5 :\n",
    "        return 2\n",
    "    if row['hhcomp'] == 6 :\n",
    "        return 2\n",
    "    if row['hhcomp'] == 7 :\n",
    "        return 2\n",
    "    if row['hhcomp'] == 8 :\n",
    "        return 3\n",
    "    if row['hhcomp'] == 9 :\n",
    "        return 3\n",
    "    if row['hhcomp'] == 10 :\n",
    "        return 3\n",
    "    if row['hhcomp'] == 11 :\n",
    "        return 4\n",
    "df_age_comp.apply (lambda row: get_adults (row), axis=1)\n",
    "df_age_comp['adults'] = df_age_comp.apply (lambda row: get_adults (row),axis=1)\n",
    "\n",
    "def get_children (row): #01012340130\n",
    "    if row['hhcomp'] == 1:\n",
    "        return 0\n",
    "    if row['hhcomp'] == 2 :\n",
    "        return 1\n",
    "    if row['hhcomp'] == 3 :\n",
    "        return 0\n",
    "    if row['hhcomp'] == 4 :\n",
    "        return 1\n",
    "    if row['hhcomp'] == 5 :\n",
    "        return 2\n",
    "    if row['hhcomp'] == 6 :\n",
    "        return 3\n",
    "    if row['hhcomp'] == 7 :\n",
    "        return 4\n",
    "    if row['hhcomp'] == 8 :\n",
    "        return 0\n",
    "    if row['hhcomp'] == 9 :\n",
    "        return 1\n",
    "    if row['hhcomp'] == 10 :\n",
    "        return 1.5\n",
    "    if row['hhcomp'] == 11 :\n",
    "        return 0\n",
    "df_age_comp.apply (lambda row: get_children (row), axis=1)\n",
    "\n",
    "df_age_comp['children'] = df_age_comp.apply (lambda row: get_children (row),axis=1)\n",
    "\n",
    "df_age_comp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_diary_with_ages = pd.merge(df_diary, df_age_comp, how='left', on='hhno')\n",
    "df_diary_with_ages.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pendiente de revisar ... y falta ver lo de las edades bien visto!!!\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = df\n",
    "df2.drop(['logday', 'purchasevalue', 'minor', 'purchasefree', 'major', 'group', 'hhcomp', 'hhcomp_text', 'hhcomp_text_long'], inplace=True, axis=1)\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Total number of adults: \", df2.adults.sum()\n",
    "print \"Total number of children: \", df2.children.sum()\n",
    "print \"Average number of adults per household: \", (df2.adults.sum()*1.0)/len(df_diary)\n",
    "print \"Average number of children per household: \", (df2.children.sum()*1.0)/len(df_diary)\n",
    "print \"Average number of people per household: \", (df2.adults.sum()*1.0)/len(df_diary) + (df2.children.sum()*1.0)/len(df_diary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#----- TIME TO FOOL AROUND!!! How much of each of the 24 groups in 2000? <===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://britains-diet.labs.theodi.org/ \"explore the data\", \"overview\", \"2000\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_of_households = len(df.household.unique())\n",
    "num_of_households"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sumfreq = df.groupby(by=['group_text'])['quantity'].sum() / num_of_households\n",
    "#But some households have more people than others\n",
    "#And foods have to be converted to 'grams per week' before they can be added\n",
    "#Or even to 'grams per day' if we are to compare with the Canaries\n",
    "#And groups should be sorted in the same way as in britains-diet\n",
    "df_sumfreq = sumfreq.to_frame()\n",
    "df_sumfreq.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "POTATOES\n",
    "FRESH GREEN VEGETABLES\n",
    "OTHER FRESH VEGETABLES\n",
    "ALL PROCESSED VEGETABLES\n",
    "ALL BREAD\n",
    "FRESH FRUIT\n",
    "FRUIT & FRUIT PRODS. NOT FRESH\n",
    "ALL NON-CARCASE MEAT AND MEAT PRODUCTS\n",
    "SUGAR AND PRESERVES\n",
    "ALL CARCASE MEAT\n",
    "ALL FATS\n",
    "ALL OTHER FOODS\n",
    "CEREALS, EXCL. BREAD,BUNS,CAKES,BISCUITS\n",
    "BISCUITS, CAKES, BUNS, CRISPBREADS\n",
    "FLOUR\n",
    "ALL FISH\n",
    "BEVERAGES\n",
    "TOTAL CHEESE\n",
    "CONFECTIONERY\n",
    "---\n",
    "LIQUID WHOLEMILK, INC SCHOOL & WELFARE\n",
    "OTHER MILK & CREAM\n",
    "SOFT DRINKS\n",
    "ALCOHOLIC DRINKS\n",
    "...rest not sorted yet...\n",
    "EGGS\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sumfreq = df_sumfreq.sort_values(by='quantity', ascending=False) #.head(20)\n",
    "my_plot = df_sumfreq.sort_values(by='quantity',ascending=True).plot(kind='barh',legend=None,title=\"Total Quantities by Food Category\")\n",
    "my_plot.set_xlabel(\"Foods\")\n",
    "my_plot.set_ylabel(\"Quantities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#----- OLDER ATTEMPTS WHICH MIGHT BE HELPFUL NOW I HAVE ONE GOOD TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df.minor_text.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sumfreq = df_merged.groupby(by=['foodcat'])['qty'].sum()\n",
    "#df_sumfreq = sumfreq.to_frame()\n",
    "#df_sumfreq.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_sumfreq = df_sumfreq.sort_values(by='qty', ascending=False).head(20)\n",
    "#my_plot = df_sumfreq.sort_values(by='qty',ascending=True).plot(kind='barh',legend=None,title=\"Total Quantities by Food Category\")\n",
    "#my_plot.set_xlabel(\"Foods\")\n",
    "#my_plot.set_ylabel(\"Quantities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pd.crosstab(index=df_merged[\"minfddesc\"], columns='cq') #gives us all the food items with a count (not a *sum* of cq!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_merged[df_merged.minfddesc == 'YOGHURT'] #3838 purchases of yoghurt, each with their own cq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_merged[df_merged.minfddesc == 'SEMI AND OTHER SKIMMED MILKS'] #10898 purchases of SEMI AND OTHER SKIMMED MILKS,\n",
    "                                                                # each with their own cq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now, add all cq's ...\n",
    "\n",
    "#this tells us how many purchases of each food\n",
    "#df_merged.minfddesc.value_counts() #10898 purchases of milk\n",
    "\n",
    "#this tells us how many purchases were of each quantity\n",
    "#df_merged[df_merged.minfddesc == 'SEMI AND OTHER SKIMMED MILKS'].cq.value_counts() #3678 purchases of milk were 1 (pint?)\n",
    "#df_merged[df_merged.minfddesc == 'YOGHURT'].cq.value_counts() #710 purchases of yoghurt were 0.87\n",
    "\n",
    "#but i'd like to say YOGHURT = so many grams in total, and I still can't do that\n",
    "#in Epiinfo it wouldbe SUMFREQ CQ MINFDDESC == each value of MINFDDESC followed by the sum of grams for that value\n",
    "#that's my task for my next 2 hour chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#major food description -> horizontal bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://pandas.pydata.org/pandas-docs/stable/visualization.html\n",
    "#http://pandas.pydata.org/pandas-docs/stable/groupby.html\n",
    "#http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\n",
    "#http://pbpython.com/simple-graphing-pandas.html <--- useful!\n",
    "#http://www.gallamine.com/2014/07/python-pandas-group-by-column-and-sum.html <--- useful!\n",
    "#http://hamelg.blogspot.co.uk/2015/11/python-for-data-analysis-part-19_17.html <--- useful!\n",
    "#http://dataconomy.com/14-best-python-pandas-features/\n",
    "#https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/\n",
    "#https://www.analyticsvidhya.com/blog/2015/04/comprehensive-guide-data-exploration-sas-using-python-numpy-scipy-matplotlib-pandas/\n",
    "#http://machinelearningmastery.com/quick-and-dirty-data-analysis-with-pandas/\n",
    "#http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from __future__ import division\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# some nicer colors from http://colorbrewer2.org/\n",
    "COLOR1 = '#7fc97f'\n",
    "COLOR2 = '#beaed4'\n",
    "COLOR3 = '#fdc086'\n",
    "COLOR4 = '#ffff99'\n",
    "COLOR5 = '#386cb0'\n",
    "\n",
    "color_list = [COLOR1, COLOR2, COLOR3, COLOR4, COLOR5]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "merged[merged.minfddesc.str.startswith('YO')].head()\n",
    "grupos = merged.groupby('minfddesc')\n",
    "\n",
    "merged.sort_values('cq', ascending = False)\n",
    "\n",
    "sums = merged.minfddesc.value_counts()\n",
    "sums\n",
    "\n",
    "totalc = len(merged)\n",
    "series_minfddesc = merged.minfddesc.value_counts() / totalc * 100\n",
    "df_minfddesc = series_minfddesc.to_frame(name=None)\n",
    "df_minfddesc.shape\n",
    "#df_minfddesc.head()\n",
    "\n",
    "df = df_minfddesc.sort_index()\n",
    "df.plot(title='Relative Frequency Distribution of minfddesc', kind='barh', color=COLOR5, grid=True)\n",
    "df.plot()\n",
    "\n",
    "#slowly getting somewhere, but i need a mentor for the python language\n",
    "#it cannot be that doing a FREQ VARIABLE is so darn difficult\n",
    "pd.crosstab(index=df[\"minfddesc\"],  # Make a crosstab\n",
    "                              columns=\"minfddesc\")      # Name the count column\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
