{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food UK - Logistic Regression\n",
    "\n",
    "- _Author_: Lucas Gonzalez Santa Cruz\n",
    "- _Workshop_: Data Science Intensive, Springboard.com\n",
    "- _Date_: 28 September 2016\n",
    "\n",
    "# tl;dr\n",
    "\n",
    "_In the UK, since 1974, several thousands of families are selected each year (different families each year). Data is collected about **each family** (about 50 variables: place, number of members of different ages, socioeconomic variables), and about **what each family buys over one week** (3 pints of skimmed milk, 2 pounds sterling; etc). Food items (\"brown bread\") are categorised into food groups (\"breads\"). For each food item, 47 **nutrients** are known (calories, vitamin D, etc)._\n",
    "\n",
    "_I'm interested in food generally, but for this project I will only:_\n",
    "- _define a score of \"shopping-cart goodness\" (more fruit+vegs than junk food)_\n",
    "- _look at how some family variables (income, location, etc) predict a \"good cart\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation and goals\n",
    "\n",
    "Food is interesting for many inter-related reasons: food security (poverty, climate change), nutritional health (blue zones), ecological reasons (edible forests), economic developement (jobs), data and automation (mapping images of fields to help growers).\n",
    "\n",
    "Here I'll start using one particular dataset with the intention of learning to use some ML procedures. Specifically, I will look at what food UK families buy, and try to predict the healthiness of their shopping from their demographic variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data source and datasets\n",
    "\n",
    "UK families (a different sample each year for many years), have been asked what food they buy for a week.\n",
    "\n",
    "Data between 1974-2000 is open data:\n",
    "* Available at http://britains-diet.labs.theodi.org\n",
    "* Datasets:\n",
    "    * Households: a file with a line per household, and for each of them 53 variables: number of female adults, male adults and children; income; location; etc. Also, many files translating codes to literals: 11=Scotland, 1=household owner is female, etc.\n",
    "    * Purchases: a file with a line per food item bought: a certain household bought, over the course of a week, 6 loaves of brown bread (money paid), 3 pints of skimmed milk (money paid), etc.\n",
    "    * Food groups: several files that put the 300+ food items (\"milk\", \"cheese\") into 20+ food groups (\"milky products\").\n",
    "    * Nutrients: for each food item (and even for 4 \"seasons\" each year) there's information about 47 nutrients (calories, vitamin D, etc).\n",
    "\n",
    "(Data since 2001 is published only in summarised form. We won't be using it here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling\n",
    "\n",
    "* We want one line per household.\n",
    "* Household variables: household identifier, year, and the other 50+ variables.\n",
    "* Also for each household: how much of each food they bought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===== IMPORTs, SETTINGS AND FUNCTIONS\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "c0=sns.color_palette()[0]\n",
    "c1=sns.color_palette()[1]\n",
    "c2=sns.color_palette()[2]\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "def points_plot(ax, Xtr, Xte, ytr, yte, clf, mesh=True, colorscale=cmap_light, cdiscrete=cmap_bold, \\\n",
    "                alpha=0.1, psize=10, zfunc=False, predicted=False):\n",
    "    h = .02\n",
    "    X=np.concatenate((Xtr, Xte))\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "\n",
    "    #plt.figure(figsize=(10,6))\n",
    "    if zfunc:\n",
    "        p0 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 0]\n",
    "        p1 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "        Z=zfunc(p0, p1)\n",
    "    else:\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    ZZ = Z.reshape(xx.shape)\n",
    "    if mesh:\n",
    "        plt.pcolormesh(xx, yy, ZZ, cmap=cmap_light, alpha=alpha, axes=ax)\n",
    "    if predicted:\n",
    "        showtr = clf.predict(Xtr)\n",
    "        showte = clf.predict(Xte)\n",
    "    else:\n",
    "        showtr = ytr\n",
    "        showte = yte\n",
    "    ax.scatter(Xtr[:, 0], Xtr[:, 1], c=showtr-1, cmap=cmap_bold, s=psize, alpha=alpha,edgecolor=\"k\")\n",
    "    # and testing points\n",
    "    ax.scatter(Xte[:, 0], Xte[:, 1], c=showte-1, cmap=cmap_bold, alpha=alpha, marker=\"s\", s=psize+10)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    return ax,xx,yy\n",
    "\n",
    "def points_plot_prob(ax, Xtr, Xte, ytr, yte, clf, colorscale=cmap_light, cdiscrete=cmap_bold, \\\n",
    "                     ccolor=cm, psize=10, alpha=0.1):\n",
    "    ax,xx,yy = points_plot(ax, Xtr, Xte, ytr, yte, clf, mesh=False, colorscale=colorscale, \\\n",
    "                           cdiscrete=cdiscrete, psize=psize, alpha=alpha, predicted=True) \n",
    "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=ccolor, alpha=.2, axes=ax)\n",
    "    cs2 = plt.contour(xx, yy, Z, cmap=ccolor, alpha=.6, axes=ax)\n",
    "    plt.clabel(cs2, fmt = '%2.1f', colors = 'k', fontsize=14, axes=ax)\n",
    "    return ax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================\n",
    "# ===== GETTING THE FILES =====\n",
    "# =============================\n",
    "\n",
    "#http://data.defra.gov.uk/Food/NationalFoodSurvey/NFSopen_2000.zip (etc for all years)\n",
    "#http://data.defra.gov.uk/Food/NationalFoodSurvey/NFSopen_Reference.zip\n",
    "data = 'data/'\n",
    "data2000 = 'data/NFSopen_2000/'\n",
    "dataRefe = 'data/NFSopen_Reference/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ===== FOOD CODES, UNITS AND GROUPS =====\n",
    "# ========================================\n",
    "\n",
    "#315 detailed \"minor\" foodcodes ...\n",
    "df_min_maj = pd.DataFrame.from_csv(dataRefe+'Ref_ Minor and major foods.txt', sep='\\t', index_col=None)\n",
    "df_min_maj.columns=['minor', 'minor_text', 'major']\n",
    "\n",
    "#... then aggregated into 184 \"major\" groups.\n",
    "df_maj_text = pd.DataFrame.from_csv(dataRefe+'Ref_ Major food codes.txt', sep='\\t', index_col=None)\n",
    "df_maj_text.columns=['major', 'major_text']\n",
    "\n",
    "#Each \"foodcode\" (all 315 of them) is described with one of 7 \"units\": pints, ounces, etc.\n",
    "df_min_units = pd.DataFrame.from_csv(dataRefe+'Ref_MINFD_Minor_food_codes.txt', sep='\\t', index_col=None)\n",
    "df_min_units.columns=['minor','minor_text','units']\n",
    "df_min_units.drop(['minor_text'], inplace=True, axis=1) #to avoid duplicate later\n",
    "\n",
    "#24 more aggregated \"groups\" were defined, and the 183 previous \"detailed groups\" were mapped to these 24.\n",
    "df_grp_text = pd.DataFrame.from_csv(dataRefe+'Ref_ food groups (standard).txt', sep='\\t', index_col=None)\n",
    "df_grp_text.columns=['group','group_text']\n",
    "\n",
    "#A \"mapping\" file with 596 rows ...\n",
    "df_mapping = pd.DataFrame.from_csv(dataRefe+'Ref_ Major-food group mapping.txt', sep='\\t', index_col=None)\n",
    "df_mapping.columns=['major','group']\n",
    "\n",
    "#... had 92 groups, and those 24 had to be selected.\n",
    "group24 = [4006, 9017, 22023, 31041, 46094, 100127, 129129, 135148, 150154, 155161, 162171, 172183, \\\n",
    "           184208, 210231, 233248, 251263, 264264, 267277, 281301, 304313, 314339, 340344, 350354, 380389]\n",
    "df_maj_group = df_mapping[df_mapping['group'].isin(group24) == True] #shape 183x2\n",
    "\n",
    "# ===== df_groups 315 groups x 6 fields: minor, minor_text, major, major_text, group, group_text\n",
    "df = pd.merge(df_min_maj, df_maj_text, how='left', on='major')\n",
    "df = pd.merge(df, df_maj_group, how='left', on='major')\n",
    "df = pd.merge(df, df_grp_text, how='left', on='group')\n",
    "df_groups = df\n",
    "\n",
    "print \"Food groups: number, number of variables, variable names\", len(df), len(df.columns), list(df.columns.values)\n",
    "\n",
    "print \"\\nHere's the number of food codes per group:\"\n",
    "df.groupby('group_text').minor.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables: 14 ['hhno', 'fooditem', 'logday', 'purchasevalue', 'minor', 'quantity', 'purchasefree', 'survyear', 'minor_text', 'major', 'units', 'major_text', 'group', 'group_text']\n",
      "DIARY ENTRIES: in 1995-2000, 42960 families bought 1581381 items, 243 foods in 24 groups.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# ===== DIARY ENTRIES =====\n",
    "# =========================\n",
    "\n",
    "df_purc1995 = pd.DataFrame.from_csv(data+'1995 diary data.txt', sep='\\t', index_col=None)\n",
    "df_purc1995['survyear'] = '1995'\n",
    "df_purc1996 = pd.DataFrame.from_csv(data+'1996 diary data.txt', sep='\\t', index_col=None)\n",
    "df_purc1996['survyear'] = '1996'\n",
    "df_purc1997 = pd.DataFrame.from_csv(data+'1997 diary data.txt', sep='\\t', index_col=None)\n",
    "df_purc1997['survyear'] = '1997'\n",
    "df_purc1998 = pd.DataFrame.from_csv(data+'1998 diary data.txt', sep='\\t', index_col=None)\n",
    "df_purc1998['survyear'] = '1998'\n",
    "df_purc1999 = pd.DataFrame.from_csv(data+'1999 diary data.txt', sep='\\t', index_col=None)\n",
    "df_purc1999['survyear'] = '1999'\n",
    "df_purc2000 = pd.DataFrame.from_csv(data+'2000 diary data.txt', sep='\\t', index_col=None)\n",
    "df_purc2000['survyear'] = '2000'\n",
    "list_of_years = [df_purc1995, df_purc1996, df_purc1997, df_purc1998, df_purc1999, df_purc2000]\n",
    "df = pd.concat(list_of_years)\n",
    "df.columns=['hhno', 'fooditem', 'logday', 'purchasevalue', 'minor', 'quantity', 'purchasefree', 'survyear']\n",
    "df_diary8 = df\n",
    "\n",
    "# ===== idem + adding \"_text\", \"group\" and \"unit\" variables\n",
    "df = pd.merge(df_diary8, df_min_maj, how='left', on='minor')\n",
    "df = pd.merge(df, df_min_units, how='left', on='minor')\n",
    "df = pd.merge(df, df_maj_text, how='left', on='major')\n",
    "df = pd.merge(df, df_maj_group, how='left', on='major')\n",
    "df = pd.merge(df, df_grp_text, how='left', on='group')\n",
    "\n",
    "# ===== DIARY WITH 14, 10 or 6 VARIABLES\n",
    "df_diary14 = df\n",
    "df_diary10 = df[['survyear', 'hhno', 'quantity', 'units', 'minor_text', \\\n",
    "                 'group_text', 'minor', 'major', 'major_text', 'group']]\n",
    "df_diary6  = df[['survyear', 'hhno', 'minor_text', 'quantity', 'units', 'group_text']]\n",
    "print \"DIARY ENTRIES: in 1995-2000, {0:4} families bought {1:6} items, {2:3} foods in {3:2} groups.\".\\\n",
    "    format(len(df.hhno.unique()), len(df), len(df.minor_text.unique()), len(df.group_text.unique()))\n",
    "\n",
    "df = df_diary14\n",
    "print \"variables and variable names:\", len(df.columns), list(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "households, number of variables and variables: 43624 54 ['hhno', 'gormet2', 'reg', 'lad', 'styr', 'stmth', 'mic', 'frez', 'owndw', 'memhh', 'schmilk', 'incgp', 'occhoh', 'sochoh', 'dacthoh', 'Finc_decile_by_members_of_hh', 'finctp', 'pernohoh', 'pernohw', 'agehoh_banded', 'agehw_banded', 'adltm', 'adltf', 'child', 'oaps', 'adltgt64', 'stqtr', 'country', 'gor', 'agemdk_banded', 'incgpa', 'hhcomp', 'hhcompa', 'hcxigs', 'dat1rec', 'doormilk', 'frij', 'earners', 'occxhoh', 'socxhoh', 'dactxhoh', 'szwkest', 'empst', 'wkdy1rec', 'vegind', 'gornw', 'pernxhoh', 'jobsthoh', 'incgp745', 'withjob', 'landlord', 'furnish', 'benefits', 'survyear']\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# ===== HOUSEHOLDS =====\n",
    "# ======================\n",
    "\n",
    "df_hous1995 = pd.DataFrame.from_csv(data+'1995 household data.txt', sep='\\t', index_col=None)\n",
    "df_hous1995['survyear'] = '1995'\n",
    "df_hous1996 = pd.DataFrame.from_csv(data+'1996 household data.txt', sep='\\t', index_col=None)\n",
    "df_hous1996['survyear'] = '1996'\n",
    "df_hous1997 = pd.DataFrame.from_csv(data+'1997 household data.txt', sep='\\t', index_col=None)\n",
    "df_hous1997['survyear'] = '1997'\n",
    "df_hous1998 = pd.DataFrame.from_csv(data+'1998 household data.txt', sep='\\t', index_col=None)\n",
    "df_hous1998['survyear'] = '1998'\n",
    "df_hous1999 = pd.DataFrame.from_csv(data+'1999 household data.txt', sep='\\t', index_col=None)\n",
    "df_hous1999['survyear'] = '1999'\n",
    "df_hous2000 = pd.DataFrame.from_csv(data+'2000 household data.txt', sep='\\t', index_col=None)\n",
    "df_hous2000['survyear'] = '2000'\n",
    "\n",
    "list_of_years = [df_hous1995, df_hous1996, df_hous1997, df_hous1998, df_hous1999, df_hous2000]\n",
    "df = pd.concat(list_of_years)\n",
    "print \"households, number of variables and variables:\", len(df), len(df.columns), list(df.columns.values)\n",
    "df_households = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===== 53 VARIABLES FOR EACH HOUSEHOLD (+ survyear):\n",
    "df_house_fields = pd.DataFrame.from_csv(dataRefe+'house-fields.txt', sep='\\t', index_col=None)\n",
    "df_house_fields.columns=['table', 'field', 'field_text', 'type1', 'size', 'type2', 'lookup', 'notes']\n",
    "df_house_fields = df_house_fields[['field', 'field_text', 'lookup', 'notes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42960, 244)\n"
     ]
    }
   ],
   "source": [
    "# ===== FOODS PER HOUSEHOLD =====\n",
    "df = df_diary14\n",
    "df_household_foods = pd.crosstab(df.hhno, df.minor_text, values = df.quantity, aggfunc=np.sum)\n",
    "df_household_foods = df_household_foods.fillna(0)\n",
    "df_household_foods.reset_index(level=0, inplace=True)\n",
    "df = df_household_foods\n",
    "#print \"FOODS PER HOUSEHOLD: shape, number of variables, and variables:\", \\\n",
    "#df.shape, len(df.columns), list(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape, number of variables, and variables: (42960, 25) 25 ['hhno', 'ALCOHOLIC DRINKS', 'ALL BREAD', 'ALL CARCASE MEAT', 'ALL FATS', 'ALL FISH', 'ALL NON-CARCASE MEAT AND MEAT PRODUCTS', 'ALL OTHER FOODS', 'ALL PROCESSED VEGETABLES', 'BEVERAGES', 'BISCUITS, CAKES, BUNS, CRISPBREADS', 'CEREALS, EXCL. BREAD,BUNS,CAKES,BISCUITS', 'CONFECTIONERY', 'EGGS', 'FLOUR', 'FRESH FRUIT', 'FRESH GREEN VEGETABLES', 'FRUIT & FRUIT PRODS. NOT FRESH', 'LIQUID WHOLEMILK, INC SCHOOL & WELFARE', 'OTHER FRESH VEGETABLES', 'OTHER MILK & CREAM', 'POTATOES', 'SOFT DRINKS', 'SUGAR AND PRESERVES', 'TOTAL CHEESE']\n"
     ]
    }
   ],
   "source": [
    "# ===== FOOD-GROUPS PER HOUSEHOLD =====\n",
    "df = df_diary14[['hhno', 'quantity', 'group_text']]\n",
    "df_household_groups = pd.crosstab(df.hhno, df.group_text, values = df.quantity, aggfunc=np.sum)\n",
    "df_household_groups = df_household_groups.fillna(0)\n",
    "df_household_groups.reset_index(level=0, inplace=True)\n",
    "df = df_household_groups\n",
    "#print \"FOOD GROUP PER HOUSEHOLD: shape, number of variables, and variables:\",\\\n",
    "#df.shape, len(df.columns), list(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So we have 1581381 diary entries from 43624 families\n",
      "and for each family we now have survyear + 53 demographical variables + 24 food-group variables.\n",
      "shape and variables: (43624, 78) 78 ['hhno', 'gormet2', 'reg', 'lad', 'styr', 'stmth', 'mic', 'frez', 'owndw', 'memhh', 'schmilk', 'incgp', 'occhoh', 'sochoh', 'dacthoh', 'Finc_decile_by_members_of_hh', 'finctp', 'pernohoh', 'pernohw', 'agehoh_banded', 'agehw_banded', 'adltm', 'adltf', 'child', 'oaps', 'adltgt64', 'stqtr', 'country', 'gor', 'agemdk_banded', 'incgpa', 'hhcomp', 'hhcompa', 'hcxigs', 'dat1rec', 'doormilk', 'frij', 'earners', 'occxhoh', 'socxhoh', 'dactxhoh', 'szwkest', 'empst', 'wkdy1rec', 'vegind', 'gornw', 'pernxhoh', 'jobsthoh', 'incgp745', 'withjob', 'landlord', 'furnish', 'benefits', 'survyear', 'ALCOHOLIC DRINKS', 'ALL BREAD', 'ALL CARCASE MEAT', 'ALL FATS', 'ALL FISH', 'ALL NON-CARCASE MEAT AND MEAT PRODUCTS', 'ALL OTHER FOODS', 'ALL PROCESSED VEGETABLES', 'BEVERAGES', 'BISCUITS, CAKES, BUNS, CRISPBREADS', 'CEREALS, EXCL. BREAD,BUNS,CAKES,BISCUITS', 'CONFECTIONERY', 'EGGS', 'FLOUR', 'FRESH FRUIT', 'FRESH GREEN VEGETABLES', 'FRUIT & FRUIT PRODS. NOT FRESH', 'LIQUID WHOLEMILK, INC SCHOOL & WELFARE', 'OTHER FRESH VEGETABLES', 'OTHER MILK & CREAM', 'POTATOES', 'SOFT DRINKS', 'SUGAR AND PRESERVES', 'TOTAL CHEESE']\n"
     ]
    }
   ],
   "source": [
    "# ===== HOUSEHOLDS WITH THEIR FOOD-GROUPS =====\n",
    "df = pd.merge(df_households, df_household_groups, how='left', on='hhno')\n",
    "df_hh_with_food_groups = df\n",
    "print \"So we have {0:5} diary entries from {1:4} families\".format(len(df_diary14), len(df_households))\n",
    "print \"and for each family we now have survyear + 53 demographical variables + 24 food-group variables.\"\n",
    "print \"shape and variables:\", df.shape, len(df.columns), list(df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "## Compute score and classification variable\n",
    "\n",
    "For each household:\n",
    "- a score of \"good food\", say dividing the amount of fruit per the amount of chocolate (or the other way round, depending who you ask!)\n",
    "- and then a categorisation of that score, say stating that a score higher than 5 is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good groups: ['LIQUID WHOLEMILK, INC SCHOOL & WELFARE', 'OTHER MILK & CREAM', 'TOTAL CHEESE', 'ALL CARCASE MEAT', 'ALL FISH', 'EGGS', 'ALL FATS', 'POTATOES', 'FRESH GREEN VEGETABLES', 'OTHER FRESH VEGETABLES', 'ALL PROCESSED VEGETABLES', 'FRESH FRUIT', 'FRUIT & FRUIT PRODS. NOT FRESH', 'ALL BREAD', 'FLOUR', 'CEREALS, EXCL. BREAD,BUNS,CAKES,BISCUITS']\n",
      "\n",
      "bad groups: ['ALCOHOLIC DRINKS', 'ALL NON-CARCASE MEAT AND MEAT PRODUCTS', 'ALL OTHER FOODS', 'BEVERAGES', 'BISCUITS, CAKES, BUNS, CRISPBREADS', 'CONFECTIONERY', 'SOFT DRINKS', 'SUGAR AND PRESERVES']\n",
      "\n",
      "all groups: ['LIQUID WHOLEMILK, INC SCHOOL & WELFARE', 'OTHER MILK & CREAM', 'TOTAL CHEESE', 'ALL CARCASE MEAT', 'ALL NON-CARCASE MEAT AND MEAT PRODUCTS', 'ALL FISH', 'EGGS', 'ALL FATS', 'SUGAR AND PRESERVES', 'POTATOES', 'FRESH GREEN VEGETABLES', 'OTHER FRESH VEGETABLES', 'ALL PROCESSED VEGETABLES', 'FRESH FRUIT', 'FRUIT & FRUIT PRODS. NOT FRESH', 'ALL BREAD', 'FLOUR', 'BISCUITS, CAKES, BUNS, CRISPBREADS', 'CEREALS, EXCL. BREAD,BUNS,CAKES,BISCUITS', 'BEVERAGES', 'ALL OTHER FOODS', 'SOFT DRINKS', 'CONFECTIONERY', 'ALCOHOLIC DRINKS']\n"
     ]
    }
   ],
   "source": [
    "# === LOW NUTRITIONAL VALUE:\n",
    "#'ALCOHOLIC DRINKS'\n",
    "#'ALL NON-CARCASE MEAT AND MEAT PRODUCTS'\n",
    "#'ALL OTHER FOODS'\n",
    "#'BEVERAGES'\n",
    "#'BISCUITS, CAKES, BUNS, CRISPBREADS'\n",
    "#'CONFECTIONERY'\n",
    "#'SOFT DRINKS'\n",
    "#'SUGAR AND PRESERVES'\n",
    "# === DOUBT:\n",
    "#'ALL BREAD' #\"WHITE\"\n",
    "#'CEREALS, EXCL. BREAD,BUNS,CAKES,BISCUITS' # (?)\n",
    "# === REST:\n",
    "#GENERALLY GOOD FOOD\n",
    "\n",
    "all_groups = df_grp_text.group_text.tolist()\n",
    "#all_groups = df_groups.group_text.unique()\n",
    "\n",
    "bad_groups = ['ALCOHOLIC DRINKS', 'ALL NON-CARCASE MEAT AND MEAT PRODUCTS', 'ALL OTHER FOODS', \\\n",
    "             'BEVERAGES', 'BISCUITS, CAKES, BUNS, CRISPBREADS', 'CONFECTIONERY', 'SOFT DRINKS', \\\n",
    "             'SUGAR AND PRESERVES']\n",
    "\n",
    "#good_groups = set(all_groups) - set(bad_groups)\n",
    "\n",
    "good_groups = [x for x in all_groups if x not in bad_groups and x is not np.nan]\n",
    "print \"good groups:\", good_groups\n",
    "print \"\\nbad groups:\", bad_groups\n",
    "#print \"\\nall groups:\", all_groups\n",
    "\n",
    "df = df_diary14[['hhno', 'quantity', 'group_text']]\n",
    "df_good = df[df.group_text.isin(good_groups)]\n",
    "df_bad = df[df.group_text.isin(bad_groups)]\n",
    "print \"All food purchases split in good and bad:\", len(df), len(df_good), len(df_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1065193, 3)\n",
      "shape, number of variables, and variables: (43624, 71) 71 ['hhno', 'gormet2', 'reg', 'lad', 'styr', 'stmth', 'mic', 'frez', 'owndw', 'memhh', 'schmilk', 'incgp', 'occhoh', 'sochoh', 'dacthoh', 'Finc_decile_by_members_of_hh', 'finctp', 'pernohoh', 'pernohw', 'agehoh_banded', 'agehw_banded', 'adltm', 'adltf', 'child', 'oaps', 'adltgt64', 'stqtr', 'country', 'gor', 'agemdk_banded', 'incgpa', 'hhcomp', 'hhcompa', 'hcxigs', 'dat1rec', 'doormilk', 'frij', 'earners', 'occxhoh', 'socxhoh', 'dactxhoh', 'szwkest', 'empst', 'wkdy1rec', 'vegind', 'gornw', 'pernxhoh', 'jobsthoh', 'incgp745', 'withjob', 'landlord', 'furnish', 'benefits', 'survyear', 'ALL BREAD', 'ALL CARCASE MEAT', 'ALL FATS', 'ALL FISH', 'ALL PROCESSED VEGETABLES', 'CEREALS, EXCL. BREAD,BUNS,CAKES,BISCUITS', 'EGGS', 'FLOUR', 'FRESH FRUIT', 'FRESH GREEN VEGETABLES', 'FRUIT & FRUIT PRODS. NOT FRESH', 'LIQUID WHOLEMILK, INC SCHOOL & WELFARE', 'OTHER FRESH VEGETABLES', 'OTHER MILK & CREAM', 'POTATOES', 'TOTAL CHEESE', 'sum_good']\n"
     ]
    }
   ],
   "source": [
    "# ===== ADD UP ALL QUANTITIES (EXCEPT HHNO) INTO SUM_GOOD\n",
    "\n",
    "df = df_good\n",
    "df_household_good_groups = pd.crosstab(df.hhno, df.group_text, values = df.quantity, aggfunc=np.sum)\n",
    "df_household_good_groups = df_household_good_groups.fillna(0)\n",
    "df_household_good_groups.reset_index(level=0, inplace=True)\n",
    "\n",
    "df = df_household_good_groups\n",
    "col_list= list(df)\n",
    "col_list.remove('hhno')\n",
    "df['sum_good'] = df[col_list].sum(axis=1)\n",
    "df_household_good_groups = df\n",
    "\n",
    "df = pd.merge(df_households, df_household_good_groups, how='left', on='hhno')\n",
    "df_hh_with_good_food_groups = df\n",
    "print \"GOOD: shape, number of variables, and variables:\", df.shape, len(df.columns), list(df.columns.values)\n",
    "\n",
    "# ===== ADD UP ALL QUANTITIES (EXCEPT HHNO) INTO SUM_BAD\n",
    "\n",
    "df = df_bad\n",
    "print df.shape\n",
    "df_household_bad_groups = pd.crosstab(df.hhno, df.group_text, values = df.quantity, aggfunc=np.sum)\n",
    "df_household_bad_groups = df_household_bad_groups.fillna(0)\n",
    "df_household_bad_groups.reset_index(level=0, inplace=True)\n",
    "df = df_household_bad_groups\n",
    "\n",
    "col_list= list(df)\n",
    "col_list.remove('hhno')\n",
    "df['sum_bad'] = df[col_list].sum(axis=1)\n",
    "df_household_bad_groups = df\n",
    "\n",
    "df = pd.merge(df_households, df_household_bad_groups, how='left', on='hhno')\n",
    "df_hh_with_bad_food_groups = df\n",
    "print \"BAD: shape, number of variables, and variables:\", df.shape, len(df.columns), list(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So we have 1581381 diary entries from 43624 families\n",
      "and for each family we now have survyear + 53 demographical variables + 24 food-group variables\n",
      "+ variables describing the goodness/badness of their shopping cart.\n",
      "Here's a sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hhno</th>\n",
       "      <th>gormet2</th>\n",
       "      <th>reg</th>\n",
       "      <th>lad</th>\n",
       "      <th>styr</th>\n",
       "      <th>stmth</th>\n",
       "      <th>mic</th>\n",
       "      <th>frez</th>\n",
       "      <th>owndw</th>\n",
       "      <th>memhh</th>\n",
       "      <th>schmilk</th>\n",
       "      <th>incgp</th>\n",
       "      <th>occhoh</th>\n",
       "      <th>sochoh</th>\n",
       "      <th>dacthoh</th>\n",
       "      <th>Finc_decile_by_members_of_hh</th>\n",
       "      <th>finctp</th>\n",
       "      <th>pernohoh</th>\n",
       "      <th>pernohw</th>\n",
       "      <th>agehoh_banded</th>\n",
       "      <th>agehw_banded</th>\n",
       "      <th>adltm</th>\n",
       "      <th>adltf</th>\n",
       "      <th>child</th>\n",
       "      <th>oaps</th>\n",
       "      <th>adltgt64</th>\n",
       "      <th>stqtr</th>\n",
       "      <th>country</th>\n",
       "      <th>gor</th>\n",
       "      <th>agemdk_banded</th>\n",
       "      <th>incgpa</th>\n",
       "      <th>hhcomp</th>\n",
       "      <th>hhcompa</th>\n",
       "      <th>hcxigs</th>\n",
       "      <th>dat1rec</th>\n",
       "      <th>doormilk</th>\n",
       "      <th>frij</th>\n",
       "      <th>earners</th>\n",
       "      <th>occxhoh</th>\n",
       "      <th>socxhoh</th>\n",
       "      <th>dactxhoh</th>\n",
       "      <th>szwkest</th>\n",
       "      <th>empst</th>\n",
       "      <th>wkdy1rec</th>\n",
       "      <th>vegind</th>\n",
       "      <th>gornw</th>\n",
       "      <th>pernxhoh</th>\n",
       "      <th>jobsthoh</th>\n",
       "      <th>incgp745</th>\n",
       "      <th>withjob</th>\n",
       "      <th>landlord</th>\n",
       "      <th>furnish</th>\n",
       "      <th>benefits</th>\n",
       "      <th>survyear</th>\n",
       "      <th>ALL BREAD</th>\n",
       "      <th>ALL CARCASE MEAT</th>\n",
       "      <th>ALL FATS</th>\n",
       "      <th>ALL FISH</th>\n",
       "      <th>ALL PROCESSED VEGETABLES</th>\n",
       "      <th>CEREALS, EXCL. BREAD,BUNS,CAKES,BISCUITS</th>\n",
       "      <th>EGGS</th>\n",
       "      <th>FLOUR</th>\n",
       "      <th>FRESH FRUIT</th>\n",
       "      <th>FRESH GREEN VEGETABLES</th>\n",
       "      <th>FRUIT &amp; FRUIT PRODS. NOT FRESH</th>\n",
       "      <th>LIQUID WHOLEMILK, INC SCHOOL &amp; WELFARE</th>\n",
       "      <th>OTHER FRESH VEGETABLES</th>\n",
       "      <th>OTHER MILK &amp; CREAM</th>\n",
       "      <th>POTATOES</th>\n",
       "      <th>TOTAL CHEESE</th>\n",
       "      <th>sum_good</th>\n",
       "      <th>ALCOHOLIC DRINKS</th>\n",
       "      <th>ALL NON-CARCASE MEAT AND MEAT PRODUCTS</th>\n",
       "      <th>ALL OTHER FOODS</th>\n",
       "      <th>BEVERAGES</th>\n",
       "      <th>BISCUITS, CAKES, BUNS, CRISPBREADS</th>\n",
       "      <th>CONFECTIONERY</th>\n",
       "      <th>SOFT DRINKS</th>\n",
       "      <th>SUGAR AND PRESERVES</th>\n",
       "      <th>sum_bad</th>\n",
       "      <th>how_good</th>\n",
       "      <th>how_bad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>224203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1995</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>220</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Wales</td>\n",
       "      <td>11</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3/1/1935 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>34.38</td>\n",
       "      <td>15.52</td>\n",
       "      <td>17.63</td>\n",
       "      <td>8.00</td>\n",
       "      <td>114.97</td>\n",
       "      <td>61.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>33.50</td>\n",
       "      <td>7.00</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>350.42</td>\n",
       "      <td>600.0</td>\n",
       "      <td>49.08</td>\n",
       "      <td>12.34</td>\n",
       "      <td>15.86</td>\n",
       "      <td>39.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>717.26</td>\n",
       "      <td>0.488554</td>\n",
       "      <td>2.046858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>224204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1995</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Wales</td>\n",
       "      <td>11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4/1/1935 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>84.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28.21</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.00</td>\n",
       "      <td>7.37</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>253.25</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.51</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.38</td>\n",
       "      <td>35.27</td>\n",
       "      <td>334.92</td>\n",
       "      <td>0.756151</td>\n",
       "      <td>1.322488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>224205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1995</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Wales</td>\n",
       "      <td>11</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5/1/1935 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>93.57</td>\n",
       "      <td>71.23</td>\n",
       "      <td>17.63</td>\n",
       "      <td>19.51</td>\n",
       "      <td>14.81</td>\n",
       "      <td>136.00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>24.88</td>\n",
       "      <td>105.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.27</td>\n",
       "      <td>10.67</td>\n",
       "      <td>80.0</td>\n",
       "      <td>8.88</td>\n",
       "      <td>780.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.15</td>\n",
       "      <td>42.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>47.27</td>\n",
       "      <td>166.09</td>\n",
       "      <td>4.696430</td>\n",
       "      <td>0.212928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     hhno  gormet2  reg  lad  styr  stmth  mic  frez  owndw  memhh  schmilk  incgp  occhoh  sochoh  dacthoh  Finc_decile_by_members_of_hh  finctp  pernohoh  pernohw  agehoh_banded  agehw_banded  adltm  adltf  child  oaps  adltgt64  stqtr country  gor  agemdk_banded  incgpa  hhcomp  hhcompa  hcxigs            dat1rec  doormilk  frij  earners  occxhoh  socxhoh  dactxhoh  szwkest  empst  wkdy1rec  vegind  gornw  pernxhoh  jobsthoh  incgp745  withjob  landlord  furnish  benefits survyear  \\\n",
       "0  224203      0.0    0   14  1995      1    1     1      6      3      0.0      1     0.0     220      0.0                            10       7         2        1            4.0           4.0      1      1      1     0         0      1   Wales   11            4.0       1       4        3       9  3/1/1935 00:00:00       0.0   0.0        1      0.0      0.0       0.0        1      2         3     0.0      2       0.0         1       0.0      0.0       0.0      0.0         0     1995   \n",
       "1  224204      0.0    0   14  1995      1    1     1      5      1      0.0      3     0.0     611      0.0                             0       7         1        1            3.0           3.0      1      0      0     0         0      1   Wales   11            3.0       2       1        1       2  4/1/1935 00:00:00       0.0   0.0        1      0.0      0.0       0.0        1      2         4     2.0      2       0.0         4       0.0      0.0       0.0      0.0         0     1995   \n",
       "2  224205      0.0    0   14  1995      1    0     0      5      2      0.0      4     0.0     221      0.0                             0       7         1        1            4.0           4.0      1      1      0     0         0      1   Wales   11            4.0       3       3        1       3  5/1/1935 00:00:00       0.0   0.0        2      0.0      0.0       0.0        1      2         5     0.0      2       0.0         5       0.0      0.0       0.0      0.0         0     1995   \n",
       "\n",
       "   ALL BREAD  ALL CARCASE MEAT  ALL FATS  ALL FISH  ALL PROCESSED VEGETABLES  CEREALS, EXCL. BREAD,BUNS,CAKES,BISCUITS  EGGS  FLOUR  FRESH FRUIT  FRESH GREEN VEGETABLES  FRUIT & FRUIT PRODS. NOT FRESH  LIQUID WHOLEMILK, INC SCHOOL & WELFARE  OTHER FRESH VEGETABLES  OTHER MILK & CREAM  POTATOES  TOTAL CHEESE  sum_good  ALCOHOLIC DRINKS  ALL NON-CARCASE MEAT AND MEAT PRODUCTS  ALL OTHER FOODS  BEVERAGES  BISCUITS, CAKES, BUNS, CRISPBREADS  CONFECTIONERY  SOFT DRINKS  SUGAR AND PRESERVES  \\\n",
       "0      34.38             15.52     17.63      8.00                    114.97                                     61.70   0.0    0.0        17.12                    0.00                            0.00                                     7.0                   33.50                7.00      33.6          0.00    350.42             600.0                                   49.08            12.34      15.86                               39.98            0.0         0.00                 0.00   \n",
       "1      84.63              0.00      0.00      0.00                     28.21                                      3.52   0.0    0.0         0.00                   14.00                            3.52                                     0.0                   32.00                7.37      80.0          0.00    253.25             200.0                                    0.00            27.51       1.76                                0.00            0.0        70.38                35.27   \n",
       "2      93.57             71.23     17.63     19.51                     14.81                                    136.00  12.0    0.0       100.00                   24.88                          105.58                                     0.0                   85.27               10.67      80.0          8.88    780.03               0.0                                   55.35             0.00      21.15                               42.32            0.0         0.00                47.27   \n",
       "\n",
       "   sum_bad  how_good   how_bad  \n",
       "0   717.26  0.488554  2.046858  \n",
       "1   334.92  0.756151  1.322488  \n",
       "2   166.09  4.696430  0.212928  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(df_households, df_household_good_groups, how='left', on='hhno')\n",
    "df = pd.merge(df, df_household_bad_groups, how='left', on='hhno')\n",
    "#len(df[df.how_good.isnull() == True]) #2315\n",
    "df = df.fillna(0)\n",
    "df['how_good'] = df['sum_good'] / df['sum_bad']\n",
    "df['how_bad'] = df['sum_bad'] / df['sum_good']\n",
    "df_hh_with_all_food_groups = df\n",
    "print \"So we have {0:5} diary entries from {1:4} families\".format(len(df_diary14), len(df_households))\n",
    "print \"and for each family we now have survyear + 53 demographical variables + 24 food-group variables\"\n",
    "print \"+ variables describing the goodness/badness of their shopping cart.\"\n",
    "print \"Here's a sample:\"\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    43624.000000\n",
      "mean       457.750644\n",
      "std        359.409462\n",
      "min          0.000000\n",
      "25%        209.847500\n",
      "50%        396.275000\n",
      "75%        624.402500\n",
      "max      13930.670000\n",
      "Name: sum_good, dtype: float64\n",
      "745 are zero\n",
      "0 are NaN\n"
     ]
    }
   ],
   "source": [
    "print df.sum_good.describe()\n",
    "print len(df[df.sum_good == 0]), \"are zero\"\n",
    "print len(df[df.sum_good == np.nan]), \"are NaN\"\n",
    "#df.sum_good.hist()\n",
    "\n",
    "print df.sum_bad.describe()\n",
    "print len(df[df.sum_bad == 0]), \"are zero\"\n",
    "print len(df[df.sum_bad == np.nan]), \"are NaN\"\n",
    "#df.sum_bad.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    4.295900e+04\n",
      "mean              inf\n",
      "std               NaN\n",
      "min      0.000000e+00\n",
      "25%               NaN\n",
      "50%               NaN\n",
      "75%               NaN\n",
      "max               inf\n",
      "Name: how_bad, dtype: float64\n",
      "count    4.295900e+04\n",
      "mean              inf\n",
      "std               NaN\n",
      "min      0.000000e+00\n",
      "25%               NaN\n",
      "50%               NaN\n",
      "75%               NaN\n",
      "max               inf\n",
      "Name: how_good, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/anaconda2/lib/python2.7/site-packages/numpy/lib/function_base.py:3403: RuntimeWarning: Invalid value encountered in median\n",
      "  RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "print df.how_bad.describe()\n",
    "print df.how_good.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It looks like I'll have to use\n",
    "# http://scikit-learn.org/stable/modules/preprocessing.html\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html\n",
    "# sklearn.preprocessing.scale(X, axis=0, with_mean=True, with_std=True, copy=True)[source]\n",
    "# I guess the matrix would be the 24 groups x 45k observations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Taken from previous exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dflog=pd.read_csv(data+\"01_heights_weights_genders.csv\")\n",
    "#dflog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#your turn\n",
    "#dflog.plot(x = 'Weight', y= 'Height', kind='scatter') #1st attempt, without colors\n",
    "#2nd attempt, with colors\n",
    "#ax = dflog[dflog.Gender == \"Male\"].plot.scatter(x='Weight', y='Height', color='DarkBlue', label='Male');\n",
    "#dflog[dflog.Gender == \"Female\"].plot.scatter(x='Weight', y='Height', color='DarkRed', label='Female', ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def cv_score(clf, x, y, score_func=accuracy_score):\n",
    "    result = 0\n",
    "    nfold = 5\n",
    "    for train, test in KFold(y.size, nfold): # split data into train/test groups, 5 times\n",
    "        clf.fit(x[train], y[train]) # fit\n",
    "        result += score_func(clf.predict(x[test]), y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dflog' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-76d31b5dcb32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m Xlr, Xtestlr, ylr, ytestlr = train_test_split(dflog[['Height','Weight']].values, \n\u001b[0m\u001b[0;32m      7\u001b[0m                                               (dflog.Gender==\"Male\").values,random_state=5)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dflog' is not defined"
     ]
    }
   ],
   "source": [
    "# First, we try a basic Logistic Regression:\n",
    "# - Split the data into a training and test (hold-out) set\n",
    "# - Train on the training set, and test for accuracy on the testing set\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "Xlr, Xtestlr, ylr, ytestlr = train_test_split(dflog[['Height','Weight']].values, \n",
    "                                              (dflog.Gender==\"Male\").values,random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(Xlr,ylr)\n",
    "print accuracy_score(clf.predict(Xtestlr),ytestlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "score = cv_score(clf, Xlr, ylr)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the grid of parameters to search over\n",
    "Cs = [0.001, 0.1, 1, 10, 100]\n",
    "\n",
    "#your turn\n",
    "for c in Cs:\n",
    "    clf = LogisticRegression(C=c)\n",
    "    clf.fit(Xlr,ylr)\n",
    "    score = accuracy_score(clf.predict(Xlr),ylr)\n",
    "    #print \"C and score: \", c, score\n",
    "#print \"(select C with maximum score, i.e. C=0.001)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#your turn\n",
    "clf = LogisticRegression(C=0.001)\n",
    "clf.fit(Xlr, ylr)\n",
    "accuracy = accuracy_score(clf.predict(Xtestlr), ytestlr)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#your turn\n",
    "from sklearn import grid_search\n",
    "clf = grid_search.GridSearchCV(LogisticRegression(C=1), param_grid = {'C':[0.001, 0.1, 1, 10, 100]})\n",
    "clf.fit(Xlr, ylr)\n",
    "#print \"best parameter:\", clf.best_params_\n",
    "#print \"with a score of:\", clf.best_score_\n",
    "#print \"From this grid of scores:\"\n",
    "#clf.grid_scores_\n",
    "\n",
    "#print \"Grid scores:\"\n",
    "#for params, mean_score, scores in clf.grid_scores_:\n",
    "#    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "#          % (mean_score, scores.std() * 2, params))\n",
    "# ??? why *2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(clf.predict(Xtestlr), ytestlr)\n",
    "print(\"Accuracy: %f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_optimize(clf, parameters, Xtrain, ytrain, n_folds=5):\n",
    "    gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds)\n",
    "    gs.fit(Xtrain, ytrain)\n",
    "    print \"BEST PARAMS\", gs.best_params_\n",
    "    best = gs.best_estimator_\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "def do_classify(clf, parameters, indf, featurenames, targetname, target1val, standardize=False, train_size=0.8):\n",
    "    subdf=indf[featurenames]\n",
    "    if standardize:\n",
    "        subdfstd=(subdf - subdf.mean())/subdf.std()\n",
    "    else:\n",
    "        subdfstd=subdf\n",
    "    X=subdfstd.values\n",
    "    y=(indf[targetname].values==target1val)*1\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, train_size=train_size)\n",
    "    clf = cv_optimize(clf, parameters, Xtrain, ytrain)\n",
    "    clf=clf.fit(Xtrain, ytrain)\n",
    "    training_accuracy = clf.score(Xtrain, ytrain)\n",
    "    test_accuracy = clf.score(Xtest, ytest)\n",
    "    print \"Accuracy on training data: %0.2f\" % (training_accuracy)\n",
    "    print \"Accuracy on test data:     %0.2f\" % (test_accuracy)\n",
    "    return clf, Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DISCUSSION\n",
    "## MAIN FINDINGS\n",
    "## LIMITATIONS\n",
    "## WHAT NEXT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
